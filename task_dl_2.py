# -*- coding: utf-8 -*-
"""Task_DL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ta1pE-UglclkNaSSFutjv-YjmFRghazx
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import re
import string
from sklearn.model_selection import train_test_split
import nltk
from nltk.corpus import stopwords

# Load dataset without header
df = pd.read_csv("/content/drive/MyDrive/Task/train_data (1).csv",
                 header=None, names=["text", "rate"])

df

# Delete the row containing 0 and 1 label
df=df.drop(index=0).reset_index(drop=True)

import re

# removing html tag using regular expression
df['text']=df['text'].astype(str).apply(lambda x: re.sub(r'<.*?>', '', x))

# after removing Html tags
print(df.head(5))

# Show full cell content (no truncation)
pd.set_option('display.max_colwidth', None)

# Print first 5 rows
print(df.head(5))

# Removing punctuation
df['text']=df['text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))
print(df.head(5))

# Removing Numbers from text column using number regular expression
df['text']=df['text'].apply(lambda x: re.sub(r'\d+', '', x))
print(df.head(5))

# Converting Lowercase
df['text']=df['text'].str.lower()
print(df.head(5))

# Using NLTK for mitigating Stopwords
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# removing stopwords
df['text']=df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))

print(df.head(5))

#importing scikitlearn
from sklearn.model_selection import train_test_split

# Spliting train as 80% and Test as 20%
X_train, X_test, y_train, y_test = train_test_split(
    df['text'],
    df['rate'],
    test_size=0.2,
    random_state=42,
    stratify=df['rate']
)


print("Train Data size:", X_train.shape[0])
print("Test Data size:", X_test.shape[0])





######## ---------- Our CUstom Hierarchical Attention Network (HAN) With word2Vec embeddings Start HERE --------------- #####

!pip install gensim torchtext
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from gensim.models import Word2Vec
from sklearn.preprocessing import LabelEncoder

# Using Word2 Vec here

from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

# tokenize the text
sentences = [word_tokenize(text.lower()) for text in X_train]

#  training word2Vec
w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)

embedding_dim = 100
word_index = {word: i+1 for i, word in enumerate(w2v_model.wv.index_to_key)}
embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))

for word, i in word_index.items():
    embedding_matrix[i] = w2v_model.wv[word]


###  Data getting ready
MAX_SENT_LEN = 30   # max words per sentence
MAX_SENTS = 10      # max sentences per document

def preprocess_text(text, word_index):
    sentences = text.split('.')[:MAX_SENTS]
    data = np.zeros((MAX_SENTS, MAX_SENT_LEN), dtype='int32')
    for i, sent in enumerate(sentences):
        words = word_tokenize(sent.lower())
        for j, word in enumerate(words[:MAX_SENT_LEN]):
            if word in word_index:
                data[i, j] = word_index[word]
    return data

X_train_indices = np.array([preprocess_text(doc, word_index) for doc in X_train])
X_test_indices = np.array([preprocess_text(doc, word_index) for doc in X_test])

# Encode labels
label_encoder = LabelEncoder()
y_train_enc = label_encoder.fit_transform(y_train)
y_test_enc = label_encoder.transform(y_test)

y_train_tensor = torch.tensor(y_train_enc)
y_test_tensor = torch.tensor(y_test_enc)



# Creating our Custom Hierarchical Attention Network (HAN)  MODEL here
class WordAttention(nn.Module):
    def __init__(self, embedding_matrix, hidden_dim):
        super(WordAttention, self).__init__()
        vocab_size, embed_dim = embedding_matrix.shape
        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), freeze=False)
        self.gru = nn.GRU(embed_dim, hidden_dim, bidirectional=True, batch_first=True)
        self.attention = nn.Linear(hidden_dim*2, 1)

    def forward(self, x):
        embedded = self.embedding(x)
        output, _ = self.gru(embedded)
        attn_weights = torch.softmax(self.attention(output), dim=1)
        sentence_vec = torch.sum(output * attn_weights, dim=1)
        return sentence_vec

class SentenceAttention(nn.Module):
    def __init__(self, hidden_dim, num_classes):
        super(SentenceAttention, self).__init__()
        self.gru = nn.GRU(hidden_dim*2, hidden_dim, bidirectional=True, batch_first=True)
        self.attention = nn.Linear(hidden_dim*2, 1)
        self.fc = nn.Linear(hidden_dim*2, num_classes)

    def forward(self, x):
        output, _ = self.gru(x)
        attn_weights = torch.softmax(self.attention(output), dim=1)
        doc_vec = torch.sum(output * attn_weights, dim=1)
        return self.fc(doc_vec)

class HAN(nn.Module):
    def __init__(self, embedding_matrix, hidden_dim, num_classes):
        super(HAN, self).__init__()
        self.word_att = WordAttention(embedding_matrix, hidden_dim)
        self.sent_att = SentenceAttention(hidden_dim, num_classes)

    def forward(self, x):
        batch_size, max_sents, max_words = x.shape
        sent_vecs = []
        for i in range(max_sents):
            sent_vec = self.word_att(x[:, i, :])
            sent_vecs.append(sent_vec.unsqueeze(1))
        sent_vecs = torch.cat(sent_vecs, dim=1)
        return self.sent_att(sent_vecs)
    

##    Training the HAN model
NUM_CLASSES = len(np.unique(y_train_enc))
hidden_dim = 50

model = HAN(embedding_matrix, hidden_dim, NUM_CLASSES)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# DataLoader
class TextDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.long)
        self.y = y

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

train_dataset = TextDataset(X_train_indices, y_train_tensor)
test_dataset = TextDataset(X_test_indices, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32)

# Training loop and defining epochs
for epoch in range(5):   # 5 epochs
    model.train()
    total_loss = 0
    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}")



# Saving the model
import joblib
# Save HAN model as .pkl
joblib.dump(model, "han_model.pkl")
print(" HAN model saved as han_model.pkl")


# Model Evaluation 
from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

model.eval()
y_pred, y_true = [], []
with torch.no_grad():
    for X_batch, y_batch in test_loader:
        outputs = model(X_batch)
        preds = torch.argmax(outputs, dim=1)
        y_pred.extend(preds.tolist())
        y_true.extend(y_batch.tolist())

print("Accuracy:", accuracy_score(y_true, y_pred))
print(classification_report(y_true, y_pred, target_names=[str(cls) for cls in label_encoder.classes_]))

# Confusion Matrix
ConfusionMatrixDisplay.from_predictions(y_true, y_pred, display_labels=[str(cls) for cls in label_encoder.classes_], cmap="Blues")
plt.show()


###########------------- Our HAN model ENDs HERE -------------##







##### --------------- Our custom  BiLSTM with Word2Vec embeddings  Model Start HERE -------------- ###
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout, Layer
import tensorflow as tf

# ------------------- DATA PREP -------------------
MAX_NUM_WORDS = 20000   # max vocabolay size
MAX_SEQUENCE_LENGTH = 100  # max length of text
EMBEDDING_DIM = 100     # embedding size

texts = df['text'].astype(str).tolist()
labels = df['rate'].values
num_classes = len(np.unique(labels))

# tokenize
tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, lower=True, oov_token="<UNK>")
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
y = to_categorical(labels, num_classes=num_classes)

# train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# ------------------- ATTENTION LAYER -------------------
class Attention(Layer):
    def __init__(self, **kwargs):
        super(Attention, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name="att_weight", shape=(input_shape[-1], 1),
                                 initializer="normal")
        self.b = self.add_weight(name="att_bias", shape=(input_shape[1], 1),
                                 initializer="zeros")
        super(Attention, self).build(input_shape)

    def call(self, x):
        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)
        a = tf.keras.backend.softmax(e, axis=1)
        output = x * a
        return tf.keras.backend.sum(output, axis=1)

# ------------------- MODEL creating -------------------
inputs = Input(shape=(MAX_SEQUENCE_LENGTH,))
embedding = Embedding(input_dim=MAX_NUM_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs)
lstm = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(embedding)
att = Attention()(lstm)
dense1 = Dense(64, activation="relu")(att)
drop = Dropout(0.5)(dense1)
outputs = Dense(num_classes, activation="softmax")(drop)

model = Model(inputs=inputs, outputs=outputs)
model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

print(model.summary())

# ------------------- TRAIN our BiLSTM model -------------------
history = model.fit(X_train, y_train,
                    batch_size=64,
                    epochs=5,
                    validation_data=(X_test, y_test))

# ----------------- Evaluation of our BiLSTM-----------------
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

y_pred_prob = model.predict(X_test)
y_pred = np.argmax(y_pred_prob, axis=1)
y_true = np.argmax(y_test, axis=1)

print("\nClassification Report:")
print(classification_report(y_true, y_pred))

# ----------------- Confusion Matrix -----------------
cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(num_classes))
disp.plot(cmap=plt.cm.Blues, values_format='d')
plt.title("Confusion Matrix - BiLSTM")
plt.show()

import joblib

# Save HAN model as .pkl
joblib.dump(model, "BiLSMT_model.pkl")







####---------------   Using Our Customize DistilBERT model Start Here -----------###
import torch
import torch.nn as nn
from transformers import DistilBertTokenizer, DistilBertModel
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# ----------------- Datas wrapping here -----------------
class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels # Keep labels as original integer indices
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        # Check if label is one-hot encoded and convert to index if necessary
        if isinstance(self.labels[idx], np.ndarray):
             label = np.argmax(self.labels[idx])
        else:
             label = self.labels[idx]

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long) # Ensure labels are Long for CrossEntropyLoss
        }

# ----------------- Creating Custom  DistilBERT Model Start here -----------------
class CustomDistilBERT(nn.Module):
    def __init__(self, num_classes, dropout_rate=0.4):
        super(CustomDistilBERT, self).__init__()
        self.bert = DistilBertModel.from_pretrained("distilbert-base-uncased")
        self.dropout1 = nn.Dropout(dropout_rate)        # our extra added Drop out 1
        self.fc1 = nn.Linear(self.bert.config.hidden_size, 256)
        self.relu = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout_rate)        # our extra added drop out 2
        self.fc2 = nn.Linear(256, num_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        hidden_state = outputs.last_hidden_state[:, 0]  #  token
        x = self.dropout1(hidden_state)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout2(x)
        return self.fc2(x)

# ----------------- Data prepartion-----------------
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
train_dataset = TextDataset(X_train, y_train, tokenizer)
test_dataset = TextDataset(X_test, y_test, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16)

# ----------------- Training Setup -----------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
num_classes = y_train.shape[1]
model = CustomDistilBERT(num_classes=num_classes).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)   # We added AdamW with own fitted learning rate

# ----------------- Training the model  -----------------
EPOCHS = 5
model.train()
for epoch in range(EPOCHS):
    total_loss = 0
    for batch in train_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(train_loader):.4f}")


# ----------------- Evaluation -----------------
model.eval()
y_true, y_pred = [], []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].cpu().numpy()
        outputs = model(input_ids, attention_mask)
        preds = torch.argmax(outputs, dim=1).cpu().numpy()

        y_true.extend(labels)
        y_pred.extend(preds)

# ----------------- Classification Report -----------------
print("\nClassification Report:")
print(classification_report(y_true, y_pred))

# ----------------- Confusion Matrix -----------------
cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(num_classes))
disp.plot(cmap=plt.cm.Blues, values_format='d')
plt.title("Confusion Matrix - Custom DistilBERT")
plt.show()

######------------------ Our custom DistilBERT model Ends HERE -------------- ########









"""
Using our Modified GPT-2 small model HERE Using pytorch But It produces CUDA
error SO this is my failure Hopefully if we get Higher GPU we will be able to run CUstom GPU for CLassification task 
"""
## ----------- CUstom GPU starts HERE ------------------- #

import torch
import torch.nn as nn
from transformers import GPT2Tokenizer, GPT2Model, GPT2Config
from torch.utils.data import Dataset, DataLoader, TensorDataset
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import numpy as np
import matplotlib.pyplot as plt

# ----------------- DEVICE -----------------
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", DEVICE)

# ----------------- Flatten & fix labels -----------------
y_train_flat = np.array(y_train).ravel().astype(int)
y_test_flat = np.array(y_test).ravel().astype(int)
num_labels = len(np.unique(y_train_flat))
print("Number of classes:", num_labels)

# ----------------- Dataset Class -----------------
class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# ----------------- Our Custom GPT-2 Model -----------------
class CustomGPT2Classifier(nn.Module):
    def __init__(self, num_labels=2, dropout_prob=0.3):
        super(CustomGPT2Classifier, self).__init__()
        config = GPT2Config.from_pretrained("gpt2")
        self.gpt2 = GPT2Model.from_pretrained("gpt2", config=config)
        self.dropout = nn.Dropout(dropout_prob)
        self.fc1 = nn.Linear(config.hidden_size, config.hidden_size // 2)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(config.hidden_size // 2, num_labels)
        self.gpt2.config.pad_token_id = self.gpt2.config.eos_token_id

    def forward(self, input_ids, attention_mask=None):
        outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)
        hidden_states = outputs.last_hidden_state

        # Mean Pooling (ignores padding)
        if attention_mask is not None:
            mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size())
            sum_hidden = torch.sum(hidden_states * mask_expanded, 1)
            mean_hidden = sum_hidden / mask_expanded.sum(1)
        else:
            mean_hidden = hidden_states.mean(dim=1)

        x = self.dropout(mean_hidden)
        x = self.fc1(x)
        x = self.relu(x)
        logits = self.fc2(x)
        return logits


# ----------------- Tokenizer -----------------
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token  # GPT2 requires pad token

# ----------------- Prepare DataLoader -----------------
train_dataset = TextDataset(X_train, y_train_flat, tokenizer)
test_dataset = TextDataset(X_test, y_test_flat, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=8)

# ----------------- Initialize Model -----------------
model = CustomGPT2Classifier(num_labels=num_labels).to(DEVICE)
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
loss_fn = nn.CrossEntropyLoss()

# ----------------- Training Loop -----------------
EPOCHS = 3
for epoch in range(EPOCHS):
    model.train()
    total_loss = 0
    for batch in train_loader:
        input_ids = batch['input_ids'].to(DEVICE)
        attention_mask = batch['attention_mask'].to(DEVICE)
        labels = batch['labels'].to(DEVICE).long()  # ensure int64 type

        optimizer.zero_grad()
        logits = model(input_ids, attention_mask)
        loss = loss_fn(logits, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(train_loader):.4f}")

# ----------------- Evaluation -----------------
model.eval()
all_preds, all_labels = [], []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch['input_ids'].to(DEVICE)
        attention_mask = batch['attention_mask'].to(DEVICE)
        labels = batch['labels'].cpu().numpy()

        logits = model(input_ids, attention_mask)
        preds = torch.argmax(logits, dim=-1).cpu().numpy()

        all_preds.extend(preds)
        all_labels.extend(labels)

# ----------------- Classification Report -----------------
print("\nClassification Report:")
print(classification_report(all_labels, all_preds))

# ----------------- Confusion Matrix -----------------
cm = confusion_matrix(all_labels, all_preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(num_labels))
disp.plot(cmap=plt.cm.Blues, values_format='d')
plt.title("Confusion Matrix - Custom GPT-2 Small")
plt.show()


##### --------------- CUstom GPU ENDs HERE ------------------#






